{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ValDXer testing\n",
    "import os\n",
    "os.environ[\"HDXER_PATH\"] = \"/home/alexi/Documents/HDXer\"\n",
    "from ValDX.ValidationDX import ValDXer\n",
    "from ValDX.VDX_Settings import Settings\n",
    "import pandas as pd\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.coordinates.XTC import XTCWriter\n",
    "\n",
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "\n",
    "settings = Settings(name='test_full0.5')\n",
    "settings.replicates = 1\n",
    "settings.gamma_range = (2,6)\n",
    "settings.train_frac = 0.5\n",
    "settings.RW_exponent = [0]\n",
    "settings.split_mode = 'R3'\n",
    "settings.stride = 1000\n",
    "# settings.HDXer_stride = 10000\n",
    "\n",
    "settings.RW_do_reweighting = True\n",
    "settings.RW_do_params = False\n",
    "import pickle\n",
    "\n",
    "VDX = ValDXer(settings)\n",
    "expt_name = 'Experimental'\n",
    "test_name = \"HOIPapo_test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### add code to read in sequence from CIF file instead of copying it manually\n",
    "\n",
    "# cif_file = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HOIP/HOIP_apo/AF-Q96EP0-F1-model_v4.cif\"\n",
    "\n",
    "# sequence_header = \"_entity_poly.pdbx_seq_one_letter_code\"\n",
    "# sequence = \"\"\n",
    "# seq_head_idx = 0\n",
    "# with open(cif_file, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for idx, line in enumerate(lines):\n",
    "#         if sequence_header in line:\n",
    "#             seq_head_idx = idx+1\n",
    "#             break\n",
    "    \n",
    "#     for idx, line in enumerate(lines[seq_head_idx:]):\n",
    "#         if idx > 0 and line[0] == \";\":\n",
    "#             break\n",
    "#         sequence += line.strip()\n",
    "\n",
    "\n",
    "# # print(sequence)\n",
    "\n",
    "\n",
    "\n",
    "# # strip sequence of non letters\n",
    "# sequence = ''.join([i for i in sequence if i.isalpha()])\n",
    "\n",
    "# print(sequence)\n",
    "\n",
    "# print(\"Sequence length: \", len(sequence))\n",
    "\n",
    "\n",
    "# # convert sequence to FASTA format\n",
    "# def write_fasta(sequence, header, file_name):\n",
    "#     \"\"\"\n",
    "#     Writes a single-letter amino acid sequence to a FASTA file.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - sequence: A string containing the amino acid sequence.\n",
    "#     - header: A string to be used as the header in the FASTA file.\n",
    "#     - file_name: The name of the FASTA file to be created.\n",
    "#     \"\"\"\n",
    "#     print(f\"Writing sequence to {file_name}\")\n",
    "#     with open(file_name, 'w') as fasta_file:\n",
    "#         # Write the header with the '>' symbol\n",
    "#         fasta_file.write(f\">{header}\\n\")\n",
    "        \n",
    "#         # Write the sequence in lines of 80 characters\n",
    "#         for i in range(0, len(sequence), 80):\n",
    "#             fasta_file.write(sequence[i:i+80] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fasta_path = os.path.join(\"raw_data\", \"HOIP\", 'HOIP_apo.fasta')\n",
    "# write_fasta(sequence, 'HOIPapo', fasta_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_hdx_path = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HOIP/HOIP_apo/HOIP_apo_peptide.csv\"\n",
    "# raw_hdx = pd.read_csv(raw_hdx_path)\n",
    "# raw_hdx.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop Unnamed: 0\t\n",
    "\n",
    "# raw_hdx = raw_hdx.drop(columns=['Unnamed: 0'])\n",
    "# raw_hdx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign peptide number for each start and end residue using ngroup\n",
    "# raw_hdx['peptide'] = raw_hdx.groupby(['Start','End']).ngroup()\n",
    "\n",
    "# raw_hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# times = [0, 0.5, 5.0]\n",
    "\n",
    "# num_peptides = len(raw_hdx)//len(times)\n",
    "\n",
    "# exposure = times * num_peptides\n",
    "\n",
    "# raw_hdx['Exposure'] = exposure\n",
    "\n",
    "# raw_hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_hdx['UptakeFraction'] = raw_hdx['Uptake'] / raw_hdx['MaxUptake']\n",
    "\n",
    "# raw_hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clamp UptakeFraction to 1\n",
    "# raw_hdx['UptakeFraction'] = raw_hdx['UptakeFraction'].clip(upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print entire dataframe\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# print(raw_hdx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # pivot exposure and uptake fraction\n",
    "# grouped = raw_hdx.pivot(index=['Start', 'End'], columns='Exposure', values='UptakeFraction').reset_index()\n",
    "\n",
    "# # drop \n",
    "# grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # print entire dataframe\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # conver to HDXer format ie start, end, exposure_1, exposure_2 \n",
    "\n",
    "# # change Start to ResStr and End to ResEnd\n",
    "# hdx = grouped.rename(columns={'Start': 'ResStr', 'End': 'ResEnd'})\n",
    "\n",
    "# # drop the exposure column\n",
    "# hdx.columns.name = None\n",
    "\n",
    "# print(hdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hdx = hdx.round(5)\n",
    "# hdx.to_csv(os.path.join(\"raw_data\", \"HOIP\", 'HOIP_apo.dat'), sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segs = hdx[['ResStr', 'ResEnd']].drop_duplicates().sort_values(by=['ResStr', 'ResEnd']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # convert to list of tuples\n",
    "# segs = [tuple(x) for x in segs.values]\n",
    "\n",
    "# print(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # write list as new lines with space delimiter\n",
    "# with open(os.path.join(\"raw_data\", \"HOIP\", 'HOIP_APO_segs.txt'), 'w') as f:\n",
    "#     for item in segs:\n",
    "#         f.write(\"%s\\n\" % ' '.join(map(str, item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### at the moment PDB fixer is adding different number of hydrogens to different structures... Need to change the code to use PROPKA to get H states and apply to all strucutres\n",
    "# BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HOIP/HOIP_apo/\"\n",
    "# sim_dir = os.path.join(BPTI_dir, \"alphafold_quick\")\n",
    "\n",
    "# pdb_list = [f for f in os.listdir(sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "# print(pdb_list) \n",
    "\n",
    "\n",
    "# H_sim_dir = os.path.join(BPTI_dir, \"alphafold_H\")\n",
    "\n",
    "# os.makedirs(H_sim_dir, exist_ok=True)\n",
    "\n",
    "# for pdb in pdb_list:\n",
    "#     continue\n",
    "#     fixer = PDBFixer(os.path.join(sim_dir, pdb))\n",
    "#     fixer.addMissingHydrogens(7.0)\n",
    "#     H_pdb_name = pdb.replace('.pdb', '_H.pdb')\n",
    "#     PDBFile.writeFile(fixer.topology, fixer.positions, open(os.path.join(H_sim_dir, H_pdb_name), 'w'), keepIds=True)\n",
    "\n",
    "# pdb_list = [f for f in os.listdir(H_sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "\n",
    "\n",
    "# top_path = os.path.join(H_sim_dir, pdb_list[0])\n",
    "# pdb_paths = [os.path.join(H_sim_dir, i) for i in pdb_list]\n",
    "\n",
    "# print(top_path)\n",
    "# print(pdb_paths)\n",
    "\n",
    "\n",
    "# small_traj_name = top_path.replace(\".pdb\",\"_small.xtc\")\n",
    "# small_traj_path = os.path.join(H_sim_dir, small_traj_name)\n",
    "\n",
    "# u = mda.Universe(top_path)\n",
    "    \n",
    "# with XTCWriter(small_traj_path, n_atoms=u.atoms.n_atoms) as W:\n",
    "#     for ts in u.trajectory:\n",
    "#         W.write(u.atoms)\n",
    "#         W.write(u.atoms)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate conformations with Alphafold\n",
    "\n",
    "# need to find out how to generate a wide range of conformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_main():\n",
    "    # BPTI data\n",
    "    BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HOIP/HOIP_apo/\"\n",
    "    BPTI_dir = \"/home/alexi/Documents/ValDX/raw_data/HOIP/HOIP_apo\"\n",
    "    # BPTI_dir = \"/home/alexi/Documents/ValDX/raw_data/HDXer_tutorial/BPTI\"\n",
    "\n",
    "    sim_name = 'HOIP_apo_AF'\n",
    "    os.listdir(BPTI_dir)\n",
    "\n",
    "    segs_name = \"HOIP_APO_segs.txt\"\n",
    "    segs_path = os.path.join(BPTI_dir, segs_name)\n",
    "\n",
    "    hdx_name = \"HOIP_apo.dat\"\n",
    "    hdx_path = os.path.join(BPTI_dir, hdx_name)\n",
    "    print(hdx_path)\n",
    "\n",
    "    rates_name = \"out__train_MD_Simulated_1Intrinsic_rates.dat\"\n",
    "    rates_path = os.path.join(BPTI_dir, rates_name)\n",
    "\n",
    "    sim_dir = os.path.join(BPTI_dir, \"alphafold_quick\")\n",
    "\n",
    "    pdb_list = [f for f in os.listdir(sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "    print(pdb_list) \n",
    "\n",
    "\n",
    "    H_sim_dir = os.path.join(BPTI_dir, \"alphafold_H\")\n",
    "\n",
    "    os.makedirs(H_sim_dir, exist_ok=True)\n",
    "\n",
    "    for pdb in pdb_list:\n",
    "        continue\n",
    "        fixer = PDBFixer(os.path.join(H_sim_dir, pdb))\n",
    "        fixer.addMissingHydrogens(7.0)\n",
    "        H_pdb_name = pdb.replace('.pdb', '_H.pdb')\n",
    "        PDBFile.writeFile(fixer.topology, fixer.positions, open(os.path.join(H_sim_dir, H_pdb_name), 'w'), keepIds=True)\n",
    "        break\n",
    "    pdb_list = [f for f in os.listdir(H_sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "\n",
    "    top_path = \"/home/alexi/Documents/ValDX/raw_data/HOIP/HOIP_apo/HOIP_apo_protonated.pdb\"\n",
    "    # pdb_paths = [os.path.join(H_sim_dir, i) for i in pdb_list]\n",
    "\n",
    "    # print(top_path)\n",
    "    # print(pdb_paths)\n",
    "\n",
    "\n",
    "    # small_traj_name = top_path.replace(\".pdb\",\"_small.xtc\")\n",
    "    # small_traj_path = os.path.join(sim_dir, small_traj_name)\n",
    "\n",
    "    # u = mda.Universe(top_path)\n",
    "        \n",
    "    # with XTCWriter(small_traj_path, n_atoms=u.atoms.n_atoms) as W:\n",
    "    #     for ts in u.trajectory:\n",
    "    #         W.write(u.atoms)\n",
    "    #         W.write(u.atoms)\n",
    "    #         break\n",
    "    # # traj_paths = [os.path.join(sim_dir, i) for i in os.listdir(sim_dir) if i.endswith(\".pdb\")]\n",
    "    \n",
    "    traj_paths = [\"/home/alexi/Documents/ValDX/raw_data/HOIP/HOIP_apo/HOIP_apo_protonated.xtc\"]\n",
    "\n",
    "    print(traj_paths)\n",
    "    return hdx_path, segs_path, rates_path, top_path, traj_paths, sim_name, expt_name, test_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdx_path, segs_path, rates_path, top_path, traj_paths, sim_name, expt_name, test_name = pre_process_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_analysis_dump, names, save_paths = VDX.run_benchmark_ensemble(system=test_name,\n",
    "                                                                    times=[0, 0.5, 5.0],\n",
    "                                                                    expt_name=expt_name,\n",
    "                                                                    n_reps=1,\n",
    "                                                                    RW=True,\n",
    "                                                                    split_mode=['r','s','R3'],\n",
    "                                                                    hdx_path=hdx_path,\n",
    "                                                                    segs_path=segs_path,\n",
    "                                                                    traj_paths=traj_paths,\n",
    "                                                                    top_path=top_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_analysis_dump, names, save_paths = VDX.run_refine_ensemble(system=test_name,\n",
    "                                                                    times=[0, 0.5, 5.0],\n",
    "                                                                    expt_name=expt_name,\n",
    "                                                                    n_reps=2,\n",
    "                                                                    split_mode=['R3'],\n",
    "                                                                    hdx_path=hdx_path,\n",
    "                                                                    segs_path=segs_path,\n",
    "                                                                    traj_paths=traj_paths,\n",
    "                                                                    top_path=top_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BPTI data\n",
    "# BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HDXer_tutorial/BPTI\"\n",
    "# # BPTI_dir = \"/home/alexi/Documents/ValDX/raw_data/HDXer_tutorial/BPTI\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expt_dir = os.path.join(BPTI_dir, \"BPTI_expt_data\")\n",
    "\n",
    "# os.listdir(expt_dir)\n",
    "\n",
    "# segs_name = \"BPTI_residue_segs.txt\"\n",
    "# segs_path = os.path.join(expt_dir, segs_name)\n",
    "\n",
    "# hdx_name = \"BPTI_expt_dfracs.dat\"\n",
    "# hdx_path = os.path.join(expt_dir, hdx_name)\n",
    "# print(hdx_path)\n",
    "\n",
    "# rates_name = \"BPTI_Intrinsic_rates.dat\"\n",
    "# rates_path = os.path.join(expt_dir, rates_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_name = 'BPTI_MD'\n",
    "\n",
    "# sim_dir = os.path.join(BPTI_dir, \"BPTI_simulations\")\n",
    "\n",
    "# os.listdir(sim_dir)\n",
    "\n",
    "# md_reps = 1\n",
    "# rep_dirs = [\"Run_\"+str(i+1) for i in range(md_reps)]\n",
    "\n",
    "# top_name = \"bpti_5pti_eq6_protonly.gro\"\n",
    "\n",
    "# top_path = os.path.join(sim_dir, rep_dirs[0], top_name)\n",
    "\n",
    "# traj_name = \"bpti_5pti_reimg_protonly.xtc\"\n",
    "\n",
    "# traj_paths = [os.path.join(sim_dir, rep_dir, traj_name) for rep_dir in rep_dirs]\n",
    "\n",
    "# print(top_path)\n",
    "# print(traj_paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_split_test(split_mode, name, system):\n",
    "\n",
    "#     # settings.split_mode = 'R'\n",
    "#     settings.split_mode = split_mode\n",
    "#     settings.name = \"_\".join([name, split_mode])\n",
    "#     settings.times = [0.0, 0.5, 5.0]\n",
    "#     VDX = ValDXer(settings)\n",
    "\n",
    "#     VDX.load_HDX_data(HDX_path=hdx_path, SEG_path=segs_path, calc_name=expt_name)\n",
    "#     # VDX.load_intrinsic_rates(path=rates_path, calc_name=expt_name)\n",
    "\n",
    "#     VDX.load_structures(top_path=top_path, traj_paths=traj_paths, calc_name=test_name)\n",
    "\n",
    "#     run_outputs = VDX.run_VDX(calc_name=test_name, expt_name=expt_name)\n",
    "#     analysis_dump, df, name = VDX.dump_analysis()\n",
    "#     save_path = VDX.save_experiment()\n",
    "\n",
    "#     return run_outputs, analysis_dump, df, name, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# splits = ['r', 'SR', 'R3']\n",
    "# split_names = ['AvsB', 'LvsX', 'mixAandB']\n",
    "# system = 'BPTITtut_test'\n",
    "\n",
    "# raw_run_outputs = {}\n",
    "# analysis_dumps = {}\n",
    "# analysis_df = pd.DataFrame()\n",
    "# names = []\n",
    "# save_paths = []\n",
    "\n",
    "\n",
    "# for split, split_name in zip(splits, split_names):\n",
    "#     run_outputs, analysis_dump, df, name, save_path = run_split_test(split, split_name, system)\n",
    "#     raw_run_outputs[name] = run_outputs\n",
    "#     analysis_dumps.update(analysis_dump)\n",
    "#     analysis_df = pd.concat([analysis_df, df])\n",
    "#     names.append(name)\n",
    "#     save_paths.append(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'your_dataframe' with your actual DataFrame variable\n",
    "df = analysis_df\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('MSE over Time by Type for each Named Split Mode')\n",
    "\n",
    "# Create boxplots\n",
    "g = g.map(sns.boxplot, \"time\", \"mse\", \"Type\", palette=\"Set3\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='Type')\n",
    "g.set_axis_labels(\"Time\", \"MSE\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'your_dataframe' with your actual DataFrame variable\n",
    "df = analysis_df\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('R over Time by Type for each Named Split Mode')\n",
    "\n",
    "# Create boxplots\n",
    "g = g.map(sns.boxplot, \"time\", \"R\", \"Type\", palette=\"Set3\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='Type')\n",
    "g.set_axis_labels(\"Time\", \"R\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot LogPfs by Residues colour by calc_name facet wrap by name\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "LogPfs = pd.concat([analysis_dumps[i][\"LogPfs\"] for i in names])\n",
    "\n",
    "print(LogPfs)\n",
    "\n",
    "LogPfs_df = LogPfs.explode(['LogPf','Residues'])\n",
    "\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(LogPfs_df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('LogPfs over Residues for each Named Split Mode')\n",
    "\n",
    "# Create lineplots\n",
    "g = g.map(sns.lineplot, \"Residues\", \"LogPf\", \"calc_name\", palette=\"Set2\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='calc_name')\n",
    "g.set_axis_labels(\"Residues\", \"LogPf\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MDAnalysis.analysis.dssp import DSSP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VDX.paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb_test = mda.Universe(top_path)\n",
    "\n",
    "# # write out as a pdb and add header\n",
    "# pdb_test.atoms.write('test.pdb')\n",
    "# with open('test.pdb', 'r') as original: data = original.read()\n",
    "# with open('test.pdb', 'w') as modified: modified.write('HEADER    '+sim_name+'\\n'+data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def PDB_to_DSSP(top_path: str, dssp_path: str=None, sim_name: str=None):\n",
    "#     \"\"\"\n",
    "#     Run DSSP on a PDB file to generate a DSSP file. Reads the output and returns a list of secondary structure elements.\n",
    "#     Secondary structure elements are reduced to a single character: H (alpha helix), S (beta sheet), or L (loop).\n",
    "#     Args:\n",
    "#     - top_path (str): The path to the topology file to create the PDB file from.\n",
    "#     - dssp_path (str): The path to save the DSSP file.\n",
    "#     - sim_name (str): Simulation name to be included in the HEADER of the PDB file.\n",
    "#     Returns:\n",
    "#     - List of tuples, each containing the residue number and its secondary structure element.\n",
    "#     \"\"\"\n",
    "#     temp_pdb = \"do_mkdssp.pdb\"\n",
    "\n",
    "#     if sim_name is None:\n",
    "#         sim_name = \"DSSP HEADER\"\n",
    "#     if dssp_path is None:\n",
    "#         dssp_path = \"dssp_file.dssp\"\n",
    "#     print(top_path)\n",
    "#     pdb_test = mda.Universe(top_path)\n",
    "\n",
    "#     # write out as a pdb and add header\n",
    "#     pdb_test.atoms.write(temp_pdb)\n",
    "\n",
    "\n",
    "#     with open(temp_pdb, 'r') as original: data = original.read()\n",
    "#     with open(temp_pdb, 'w') as modified: modified.write('HEADER    '+sim_name+'\\n'+data)\n",
    "\n",
    "#     # Run mkdssp to generate DSSP file\n",
    "#     try:\n",
    "#         subprocess.run(['mkdssp', temp_pdb,  dssp_path], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Error running DSSP: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Parse the DSSP file\n",
    "#     secondary_structures = []\n",
    "#     with open(dssp_path, 'r') as dssp_file:\n",
    "#         # Skip header lines\n",
    "#         for line in dssp_file:\n",
    "#             if line.startswith('  #  RESIDUE AA'):\n",
    "#                 break\n",
    "#         # Read the secondary structure assignments\n",
    "#         for line in dssp_file:\n",
    "#             if len(line) > 13:  # Ensure line has enough data\n",
    "#                 residue_num = line[5:10].strip()\n",
    "#                 ss = line[16]\n",
    "#                 # Simplify the secondary structure to H, S, or L\n",
    "#                 if ss in 'GHI':\n",
    "#                     ss = 'H'  # Helix\n",
    "#                 elif ss in 'EB':\n",
    "#                     ss = 'S'  # Sheet\n",
    "#                 else:\n",
    "#                     ss = 'L'  # Loop or other\n",
    "#                 secondary_structures.append((residue_num, ss))\n",
    "\n",
    "#     # Cleanup temp PDB file\n",
    "#     os.remove(temp_pdb)\n",
    "#     os.remove(dssp_path)\n",
    "#     print(len(secondary_structures))\n",
    "#     print(len(pdb_test.residues))\n",
    "#     return secondary_structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDXER_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
