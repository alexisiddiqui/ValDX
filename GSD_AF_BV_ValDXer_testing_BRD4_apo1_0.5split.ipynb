{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ValDXer testing\n",
    "import os\n",
    "from ValDX.ValidationDX import ValDXer\n",
    "from ValDX.VDX_Settings import Settings\n",
    "import pandas as pd\n",
    "import MDAnalysis as mda\n",
    "from MDAnalysis.coordinates.XTC import XTCWriter\n",
    "\n",
    "\n",
    "settings = Settings(name='test_full0.5')\n",
    "settings.replicates = 1\n",
    "settings.gamma_range = (2,6)\n",
    "settings.train_frac = 0.5\n",
    "settings.RW_exponent = [0]\n",
    "settings.split_mode = 'R3'\n",
    "settings.stride = 1000\n",
    "# settings.HDXer_stride = 10000\n",
    "\n",
    "settings.RW_do_reweighting = False\n",
    "settings.RW_do_params = True\n",
    "import pickle\n",
    "\n",
    "VDX = ValDXer(settings)\n",
    "expt_name = 'Experimental'\n",
    "test_name = \"BRD4apo1_test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add code to read in sequence from CIF file instead of copying it manually\n",
    "\n",
    "cif_file = \"raw_data/BRD4/BRD4_APO/AF-O60885-F1-model_v4.cif\"\n",
    "\n",
    "sequence_header = \"_entity_poly.pdbx_seq_one_letter_code\"\n",
    "sequence = \"\"\n",
    "seq_head_idx = 0\n",
    "with open(cif_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for idx, line in enumerate(lines):\n",
    "        if sequence_header in line:\n",
    "            seq_head_idx = idx+1\n",
    "            break\n",
    "    \n",
    "    for idx, line in enumerate(lines[seq_head_idx:]):\n",
    "        if idx > 0 and line[0] == \";\":\n",
    "            break\n",
    "        sequence += line.strip()\n",
    "\n",
    "\n",
    "# print(sequence)\n",
    "\n",
    "\n",
    "\n",
    "# strip sequence of non letters\n",
    "sequence = ''.join([i for i in sequence if i.isalpha()])\n",
    "\n",
    "print(sequence)\n",
    "\n",
    "print(\"Sequence length: \", len(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequence to FASTA format\n",
    "def write_fasta(sequence, header, file_name):\n",
    "    \"\"\"\n",
    "    Writes a single-letter amino acid sequence to a FASTA file.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequence: A string containing the amino acid sequence.\n",
    "    - header: A string to be used as the header in the FASTA file.\n",
    "    - file_name: The name of the FASTA file to be created.\n",
    "    \"\"\"\n",
    "    print(f\"Writing sequence to {file_name}\")\n",
    "    with open(file_name, 'w') as fasta_file:\n",
    "        # Write the header with the '>' symbol\n",
    "        fasta_file.write(f\">{header}\\n\")\n",
    "        \n",
    "        # Write the sequence in lines of 80 characters\n",
    "        for i in range(0, len(sequence), 80):\n",
    "            fasta_file.write(sequence[i:i+80] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_path = os.path.join(\"raw_data\", \"BRD4\", 'BRD4_APO.fasta')\n",
    "write_fasta(sequence, 'LXRa', fasta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hdx_path = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/BRD4/BRD4_APO/ELN55049_AllResultsTables_Curated.csv\"\n",
    "raw_hdx = pd.read_csv(raw_hdx_path)\n",
    "raw_hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert FD in DeutTime to -1\n",
    "raw_hdx[\"Exposure\"] = raw_hdx[\"DeutTime\"].replace('FD', -1)\n",
    "\n",
    "# remove 's' from Deuteration Time\n",
    "raw_hdx[\"Exposure\"] = raw_hdx[\"Exposure\"].str.replace('s', '').astype(float)\n",
    "\n",
    "# replace NaN with -1\n",
    "raw_hdx[\"Exposure\"].fillna(-1, inplace=True)\n",
    "\n",
    "raw_hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print entire dataframe\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "print(raw_hdx.loc[raw_hdx[\"Exposure\"] == 0][\"Uptake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_hdx.loc[raw_hdx[\"Exposure\"] == 0].Uptake.value_counts(dropna=False))\n",
    "\n",
    "# fill NaNs with 0\n",
    "raw_hdx[\"Uptake\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â group by Start and End to extract peptide using ngroup\n",
    "raw_hdx[\"Peptide\"] = raw_hdx.groupby([\"Start\", \"End\"]).ngroup()\n",
    "\n",
    "raw_hdx.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average Uptake for each peptide and Exposure\n",
    "hdx = raw_hdx.groupby([\"Start\",\"End\",\"Peptide\", \"Exposure\"])[\"Uptake\"].mean().reset_index()\n",
    "\n",
    "print(hdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select Exposure -1\n",
    "max_uptake = hdx.loc[hdx[\"Exposure\"] == -1][\"Uptake\"].values\n",
    "\n",
    "print(max_uptake)\n",
    "\n",
    "no_exposure_times = hdx[\"Exposure\"].unique()\n",
    "print(len(no_exposure_times))\n",
    "\n",
    "# extend max_uptake to all Exposure times (each elemetn should be repeated len(no_exposure_times) times) ie [[m]*no_exposure_times for m in max_uptake]\n",
    "max_uptake = [m for m in max_uptake for _ in range(len(no_exposure_times))]\n",
    "print(max_uptake)\n",
    "\n",
    "\n",
    "# add max_uptake to hdx\n",
    "hdx[\"MaxUptake\"] = max_uptake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdx['UptakeFraction'] = hdx['Uptake'] / hdx['MaxUptake']\n",
    "\n",
    "hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Exposure -1\n",
    "hdx = hdx.loc[hdx[\"Exposure\"] != -1]\n",
    "\n",
    "hdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pivot exposure and uptake fraction\n",
    "hdx = hdx.groupby(['Start', 'End', 'Exposure'])['UptakeFraction'].mean().reset_index()\n",
    "\n",
    "print(hdx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clamp UptakeFraction to 1\n",
    "hdx[\"UptakeFraction\"] = hdx[\"UptakeFraction\"].clip(upper=1)\n",
    "print(hdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to HDXer format ie start, end, exposure_1, exposure_2 \n",
    "\n",
    "# pivot so that exposure time is the column name drop the exposure column\n",
    "hdx = hdx.pivot(index=['Start', 'End'], columns='Exposure', values='UptakeFraction').reset_index()\n",
    "\n",
    "# change Start to ResStr and End to ResEnd\n",
    "hdx = hdx.rename(columns={'Start': 'ResStr', 'End': 'ResEnd'})\n",
    "\n",
    "# drop the exposure column\n",
    "hdx.columns.name = None\n",
    "\n",
    "print(hdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdx = hdx.round(5)\n",
    "hdx.to_csv(os.path.join(\"raw_data\", \"BRD4\", 'BRD4_APO.dat'), sep=' ', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = raw_hdx[['Start', 'End']].drop_duplicates().sort_values(by=['Start', 'End']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert to list of tuples\n",
    "segs = [tuple(x) for x in segs.values]\n",
    "\n",
    "print(segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# write list as new lines with space delimiter\n",
    "with open(os.path.join(\"raw_data\", \"BRD4\", 'BRD4_APO_segs.txt'), 'w') as f:\n",
    "    for item in segs:\n",
    "        f.write(\"%s\\n\" % ' '.join(map(str, item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "\n",
    "BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/BRD4/BRD4_APO\"\n",
    "sim_dir = os.path.join(BPTI_dir, \"alphafold_quick\")\n",
    "\n",
    "pdb_list = [f for f in os.listdir(sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "print(pdb_list) \n",
    "\n",
    "\n",
    "H_sim_dir = os.path.join(BPTI_dir, \"alphafold_H\")\n",
    "\n",
    "os.makedirs(H_sim_dir, exist_ok=True)\n",
    "\n",
    "for pdb in pdb_list:\n",
    "    fixer = PDBFixer(os.path.join(sim_dir, pdb))\n",
    "    fixer.addMissingHydrogens(7.0)\n",
    "    H_pdb_name = pdb.replace('.pdb', '_H.pdb')\n",
    "    PDBFile.writeFile(fixer.topology, fixer.positions, open(os.path.join(H_sim_dir, H_pdb_name), 'w'), keepIds=True)\n",
    "\n",
    "pdb_list = [f for f in os.listdir(H_sim_dir) if f.endswith('.pdb')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate conformations with Alphafold\n",
    "\n",
    "# need to find out how to generate a wide range of conformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_main():\n",
    "    # BPTI data\n",
    "    BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/BRD4/BRD4_APO\"\n",
    "\n",
    "    # BPTI_dir = \"/home/alexi/Documents/ValDX/raw_data/HDXer_tutorial/BPTI\"\n",
    "\n",
    "    os.listdir(BPTI_dir)\n",
    "\n",
    "    segs_name = \"BRD4_APO_segs.txt\"\n",
    "    segs_path = os.path.join(BPTI_dir, segs_name)\n",
    "\n",
    "    hdx_name = \"BRD4_APO.dat\"\n",
    "    hdx_path = os.path.join(BPTI_dir, hdx_name)\n",
    "    print(hdx_path)\n",
    "\n",
    "    rates_name = \"out__train_MD_Simulated_1Intrinsic_rates.dat\"\n",
    "    rates_path = os.path.join(BPTI_dir, rates_name)\n",
    "    sim_name = 'BRD4_AF'\n",
    "\n",
    "    sim_dir = os.path.join(BPTI_dir, \"alphafold_quick\")\n",
    "\n",
    "    pdb_list = [f for f in os.listdir(sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "    print(pdb_list) \n",
    "\n",
    "\n",
    "    H_sim_dir = os.path.join(BPTI_dir, \"alphafold_H\")\n",
    "\n",
    "    os.makedirs(H_sim_dir, exist_ok=True)\n",
    "\n",
    "    for pdb in pdb_list:\n",
    "        fixer = PDBFixer(os.path.join(sim_dir, pdb))\n",
    "        fixer.addMissingHydrogens(7.0)\n",
    "        H_pdb_name = pdb.replace('.pdb', '_H.pdb')\n",
    "        PDBFile.writeFile(fixer.topology, fixer.positions, open(os.path.join(H_sim_dir, H_pdb_name), 'w'), keepIds=True)\n",
    "\n",
    "    pdb_list = [f for f in os.listdir(H_sim_dir) if f.endswith('.pdb')]\n",
    "\n",
    "\n",
    "    top_path = os.path.join(H_sim_dir, pdb_list[0])\n",
    "    pdb_paths = [os.path.join(H_sim_dir, i) for i in pdb_list]\n",
    "\n",
    "    print(top_path)\n",
    "    print(pdb_paths)\n",
    "\n",
    "\n",
    "    small_traj_name = top_path.replace(\".pdb\",\"_small.xtc\")\n",
    "    small_traj_path = os.path.join(sim_dir, small_traj_name)\n",
    "\n",
    "    u = mda.Universe(top_path, pdb_paths)\n",
    "\n",
    "\n",
    "        \n",
    "    with XTCWriter(small_traj_path, n_atoms=u.atoms.n_atoms) as W:\n",
    "        for ts in u.trajectory:\n",
    "                W.write(u.atoms)\n",
    "\n",
    "    # traj_paths = [os.path.join(sim_dir, i) for i in os.listdir(sim_dir) if i.endswith(\".pdb\")]\n",
    "    \n",
    "    traj_paths = [small_traj_path]\n",
    "\n",
    "    print(traj_paths)\n",
    "    return hdx_path, segs_path, rates_path, top_path, traj_paths, sim_name, expt_name, test_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdx_path, segs_path, rates_path, top_path, traj_paths, sim_name, expt_name, test_name = pre_process_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BPTI data\n",
    "# BPTI_dir = \"/Users/alexi/Library/CloudStorage/OneDrive-Nexus365/Rotation_Projects/Rotation_3/Project/ValDX/raw_data/HDXer_tutorial/BPTI\"\n",
    "# # BPTI_dir = \"/home/alexi/Documents/ValDX/raw_data/HDXer_tutorial/BPTI\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expt_dir = os.path.join(BPTI_dir, \"BPTI_expt_data\")\n",
    "\n",
    "# os.listdir(expt_dir)\n",
    "\n",
    "# segs_name = \"BPTI_residue_segs.txt\"\n",
    "# segs_path = os.path.join(expt_dir, segs_name)\n",
    "\n",
    "# hdx_name = \"BPTI_expt_dfracs.dat\"\n",
    "# hdx_path = os.path.join(expt_dir, hdx_name)\n",
    "# print(hdx_path)\n",
    "\n",
    "# rates_name = \"BPTI_Intrinsic_rates.dat\"\n",
    "# rates_path = os.path.join(expt_dir, rates_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_name = 'BPTI_MD'\n",
    "\n",
    "# sim_dir = os.path.join(BPTI_dir, \"BPTI_simulations\")\n",
    "\n",
    "# os.listdir(sim_dir)\n",
    "\n",
    "# md_reps = 1\n",
    "# rep_dirs = [\"Run_\"+str(i+1) for i in range(md_reps)]\n",
    "\n",
    "# top_name = \"bpti_5pti_eq6_protonly.gro\"\n",
    "\n",
    "# top_path = os.path.join(sim_dir, rep_dirs[0], top_name)\n",
    "\n",
    "# traj_name = \"bpti_5pti_reimg_protonly.xtc\"\n",
    "\n",
    "# traj_paths = [os.path.join(sim_dir, rep_dir, traj_name) for rep_dir in rep_dirs]\n",
    "\n",
    "# print(top_path)\n",
    "# print(traj_paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split_test(split_mode, name, system):\n",
    "\n",
    "    # settings.split_mode = 'R'\n",
    "    settings.split_mode = split_mode\n",
    "    settings.name = \"_\".join([name, split_mode])\n",
    "    settings.times = [0.0, 15.0, 60.0, 600.0, 3600.0, 14400.0]\n",
    "    VDX = ValDXer(settings)\n",
    "\n",
    "    VDX.load_HDX_data(HDX_path=hdx_path, SEG_path=segs_path, calc_name=expt_name)\n",
    "    # VDX.load_intrinsic_rates(path=rates_path, calc_name=expt_name)\n",
    "\n",
    "    VDX.load_structures(top_path=top_path, traj_paths=traj_paths, calc_name=test_name)\n",
    "\n",
    "    run_outputs = VDX.run_VDX(calc_name=test_name, expt_name=expt_name)\n",
    "    analysis_dump, df, name = VDX.dump_analysis()\n",
    "    save_path = VDX.save_experiment()\n",
    "\n",
    "    return run_outputs, analysis_dump, df, name, save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "splits = ['S', 'SR', 'Sp']\n",
    "split_names = ['AvsB', 'LvsX', 'mixAandB']\n",
    "system = 'BPTITtut_test'\n",
    "\n",
    "raw_run_outputs = {}\n",
    "analysis_dumps = {}\n",
    "analysis_df = pd.DataFrame()\n",
    "names = []\n",
    "save_paths = []\n",
    "\n",
    "\n",
    "for split, split_name in zip(splits, split_names):\n",
    "    run_outputs, analysis_dump, df, name, save_path = run_split_test(split, split_name, system)\n",
    "    raw_run_outputs[name] = run_outputs\n",
    "    analysis_dumps.update(analysis_dump)\n",
    "    analysis_df = pd.concat([analysis_df, df])\n",
    "    names.append(name)\n",
    "    save_paths.append(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'your_dataframe' with your actual DataFrame variable\n",
    "df = analysis_df\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('MSE over Time by Type for each Named Split Mode')\n",
    "\n",
    "# Create boxplots\n",
    "g = g.map(sns.boxplot, \"time\", \"mse\", \"Type\", palette=\"Set3\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='Type')\n",
    "g.set_axis_labels(\"Time\", \"MSE\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Replace 'your_dataframe' with your actual DataFrame variable\n",
    "df = analysis_df\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('R over Time by Type for each Named Split Mode')\n",
    "\n",
    "# Create boxplots\n",
    "g = g.map(sns.boxplot, \"time\", \"R\", \"Type\", palette=\"Set3\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='Type')\n",
    "g.set_axis_labels(\"Time\", \"R\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot LogPfs by Residues colour by calc_name facet wrap by name\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "LogPfs = pd.concat([analysis_dumps[i][\"LogPfs\"] for i in names])\n",
    "\n",
    "print(LogPfs)\n",
    "\n",
    "LogPfs_df = LogPfs.explode(['LogPf','Residues'])\n",
    "\n",
    "\n",
    "# Create a FacetGrid, using 'name' for each subplot\n",
    "g = sns.FacetGrid(LogPfs_df, col=\"name\", col_wrap=3, height=4, aspect=1.5)\n",
    "g.fig.suptitle('LogPfs over Residues for each Named Split Mode')\n",
    "\n",
    "# Create lineplots\n",
    "g = g.map(sns.lineplot, \"Residues\", \"LogPf\", \"calc_name\", palette=\"Set2\")\n",
    "\n",
    "# Adding some additional options for better visualization\n",
    "g.add_legend(title='calc_name')\n",
    "g.set_axis_labels(\"Residues\", \"LogPf\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MDAnalysis.analysis.dssp import DSSP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VDX.paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb_test = mda.Universe(top_path)\n",
    "\n",
    "# # write out as a pdb and add header\n",
    "# pdb_test.atoms.write('test.pdb')\n",
    "# with open('test.pdb', 'r') as original: data = original.read()\n",
    "# with open('test.pdb', 'w') as modified: modified.write('HEADER    '+sim_name+'\\n'+data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def PDB_to_DSSP(top_path: str, dssp_path: str=None, sim_name: str=None):\n",
    "#     \"\"\"\n",
    "#     Run DSSP on a PDB file to generate a DSSP file. Reads the output and returns a list of secondary structure elements.\n",
    "#     Secondary structure elements are reduced to a single character: H (alpha helix), S (beta sheet), or L (loop).\n",
    "#     Args:\n",
    "#     - top_path (str): The path to the topology file to create the PDB file from.\n",
    "#     - dssp_path (str): The path to save the DSSP file.\n",
    "#     - sim_name (str): Simulation name to be included in the HEADER of the PDB file.\n",
    "#     Returns:\n",
    "#     - List of tuples, each containing the residue number and its secondary structure element.\n",
    "#     \"\"\"\n",
    "#     temp_pdb = \"do_mkdssp.pdb\"\n",
    "\n",
    "#     if sim_name is None:\n",
    "#         sim_name = \"DSSP HEADER\"\n",
    "#     if dssp_path is None:\n",
    "#         dssp_path = \"dssp_file.dssp\"\n",
    "#     print(top_path)\n",
    "#     pdb_test = mda.Universe(top_path)\n",
    "\n",
    "#     # write out as a pdb and add header\n",
    "#     pdb_test.atoms.write(temp_pdb)\n",
    "\n",
    "\n",
    "#     with open(temp_pdb, 'r') as original: data = original.read()\n",
    "#     with open(temp_pdb, 'w') as modified: modified.write('HEADER    '+sim_name+'\\n'+data)\n",
    "\n",
    "#     # Run mkdssp to generate DSSP file\n",
    "#     try:\n",
    "#         subprocess.run(['mkdssp', temp_pdb,  dssp_path], check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Error running DSSP: {e}\")\n",
    "#         return []\n",
    "\n",
    "#     # Parse the DSSP file\n",
    "#     secondary_structures = []\n",
    "#     with open(dssp_path, 'r') as dssp_file:\n",
    "#         # Skip header lines\n",
    "#         for line in dssp_file:\n",
    "#             if line.startswith('  #  RESIDUE AA'):\n",
    "#                 break\n",
    "#         # Read the secondary structure assignments\n",
    "#         for line in dssp_file:\n",
    "#             if len(line) > 13:  # Ensure line has enough data\n",
    "#                 residue_num = line[5:10].strip()\n",
    "#                 ss = line[16]\n",
    "#                 # Simplify the secondary structure to H, S, or L\n",
    "#                 if ss in 'GHI':\n",
    "#                     ss = 'H'  # Helix\n",
    "#                 elif ss in 'EB':\n",
    "#                     ss = 'S'  # Sheet\n",
    "#                 else:\n",
    "#                     ss = 'L'  # Loop or other\n",
    "#                 secondary_structures.append((residue_num, ss))\n",
    "\n",
    "#     # Cleanup temp PDB file\n",
    "#     os.remove(temp_pdb)\n",
    "#     os.remove(dssp_path)\n",
    "#     print(len(secondary_structures))\n",
    "#     print(len(pdb_test.residues))\n",
    "#     return secondary_structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDXER_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
